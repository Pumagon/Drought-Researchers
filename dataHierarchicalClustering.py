# -*- coding: utf-8 -*-
"""Nasa Hackathon Clustering 3 fetures

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-yXrL3jNfT0zF803zKPapsrLF9GoUw1I
"""

import requests, csv, os, datetime, pprint
from geopy.geocoders import Nominatim
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
import matplotlib.pyplot as plt
import plotly.express as px
import pandas as pd

# All parameters are set in this cell.
cities = ["Seattle, WA", "Bellevue, WA", "Renton, WA", "Issaquah, WA", "North Bend, WA",
           "Easton, WA", "Cle Elum, WA", "Ellensburg, WA", "Quincy, WA",
          "George, WA", "Ritzville, WA", "Sprague, WA", "Cheney, WA", "Spokane, WA", "Liberty Lake, WA"]
startDate = 20210601
endDate = 20220601
features = ["EVLAND", "PRECTOTCORR"]
            # "T2M_MAX"]

outputFileName = "features.csv"
outputFileName2 = "features-clustered.csv"
elevation = 0
latLons = []
outputRows = []
params = ",".join(features)
geolocator = Nominatim(user_agent="spaceapps23")

# Takes "cityName, stateCode" as cityState and returns its (lat, lon).
def cityStateToLatLon(cityState):
    queryParams = {"city" : cityState.split(",")[0].lstrip().rstrip(),
                   "state" : cityState.split(",")[1].lstrip().rstrip(),
                   "country" : "United States" }
    location = geolocator.geocode(query=queryParams, timeout=1000)
    if location != None:
        return (location.latitude, location.longitude)
    else:
        return (None, None)

# Downloads data from NASA POWER for cityState (i.e. "cityName, stateCode").
# Generates a CSV file for the cityState (e.g. Boston-MA.csv).
def downloadDataForCity(cityState, startDate, endDate, params):
    lat, lon = cityStateToLatLon(cityState)
    city = cityState.split(",")[0].lstrip().rstrip()
    state = cityState.split(",")[1].lstrip().rstrip()
    url = "https://power.larc.nasa.gov/api/temporal/daily/point?parameters=" + \
            str(params) + "&community=AG&" + \
            "longitude=" + str(lon) + "&latitude=" + str(lat) + \
            "&start=" + str(startDate) + "&end=" + str(endDate) + "&format=CSV"
    response = requests.get(url)
    if response.status_code != 200:
        print("Failed to download data from NASA POWER.",
              response.status_code, (lat,lon))
        return (None, lat, lon)
    elif response.status_code == 200:
        print("Downloaded data from NASA POWER for", cityState, (lat,lon))
        downloadedFile = city + "-" + state + ".csv"
        with open(downloadedFile, "wb") as f:
            f.write(response.content)
        return (downloadedFile, lat, lon)

# Extracts elevation data from csvFileName (a CSV file downloaded from NASA POWER).
def getElevation(csvFileName):
    elevation = None
    with open(csvFileName, "r") as f:
        csvReader = csv.reader(f)
        for row in csvReader:
            if row[0].startswith("Elevation"):
                elevation = row[0].split("= ")[1].split(" ")[0]
    return elevation

# Removes the header section from csvFileName (a CSV file downloaded from NASA POWER).
# Saves remaining data in a new CSV file (e.g. Boston-MA-cleaned.csv)
def cleanCsvFile(csvFileName):
    rows =[]
    with open(csvFileName, "r") as f:
        csvReader = csv.reader(f)
        # Remove the header lines
        for row in csvReader:
            if row[0] == "-END HEADER-":
                break
        # Copy each remaining row to a new CSV file
        for row in csvReader:
            rows.append(row)
        outputFileName = csvFileName[:-4] + "-" + "cleaned.csv"
        with open(outputFileName, 'w') as f:
            writer = csv.writer(f)
            writer.writerows(rows)
    return outputFileName

# Takes a "cleaned" CSV file and calculates the mean of each feature.
# Returns a list of mean feature values.
def getMeanFeatureValues(csvFileName, features):
    featureSum = [0] * len(features)
    featureMissingCount = [0] * len(features)
    meanFeatures = []

    with open(csvFileName, "r") as f:
        csvReader = csv.reader(f)
        firstLine = next(csvReader)
        if firstLine[2:] != features:
            print("The expected features are not included in an input file.")
            return None
        else:
            for rowindex, row in enumerate(csvReader):
                for i, feature in enumerate(features):
                    if float( row[2+i] ) != -999:
                        featureSum[i] += float(row[2+i])
                    else:
                        featureMissingCount[i] += featureMissingCount[i]
            for i, feature in enumerate(features):
                meanFeatures.append(
                    featureSum[i]/(rowindex+1-featureMissingCount[i]) )
            return meanFeatures

# Generates an output CSV file (as outputFileName)
outputCsvRows = []
with open(outputFileName, "w", newline="") as f:
    writer = csv.writer(f)
    writer.writerow( ["Location", "Lat", "Lon", "Elevation"] + features )

    for city in cities:
        csvFileName, lat, lon = downloadDataForCity(city, startDate, endDate, params)
        elevation = getElevation(csvFileName)
        cleanedCsvFileName = cleanCsvFile(csvFileName)
        featureValues = getMeanFeatureValues(cleanedCsvFileName, features)
        outputCsvRows.append([city, lat, lon, elevation] + featureValues)
        print("Feature values generated and saved for ", city, (lat, lon))
    writer.writerows(outputCsvRows)
print("All feature values are saved in ", outputFileName)

# Reads outputFileName and shows a scatter diagram.
entireDataSet = [] # str elements
locations = []     # str elements
featureMatrix = [] # float elements
with open(outputFileName, "r") as f:
    csvReader = csv.reader(f)
    firstRow = next(csvReader)
    entireDataSet.append(firstRow)
    for row in csvReader:
        entireDataSet.append(row)
        locations.append(row[0])
        featureMatrix.append( [float(i) for i in row[3:]] )

df = pd.read_csv(outputFileName)
# print(df)
scatterFig1 = px.scatter_3d(df,
                    x=entireDataSet[0][3],
                    y=entireDataSet[0][4],
                    z=entireDataSet[0][5],
                    hover_name=entireDataSet[0][0] )
scatterFig1.update_layout(height=800)
scatterFig1.show()

# Normalizes values in featureMatrix. Does min-max normalization
def minMaxNormalization(floatList):
    minVal = min(floatList)
    maxVal = max(floatList)
    return [(elem - minVal) / (maxVal - minVal) for elem in floatList]

normalizedFeatureMatrix = \
    list(zip(*[minMaxNormalization(featureVals) for featureVals in list(zip(*featureMatrix))]))

# Performs hierarchical clustering with normalizedFeatureMatrix and shows its result as a dendrogram
Z = linkage(normalizedFeatureMatrix, metric="euclidean", method="ward")
# print(Z)
plt.figure(figsize=(10,8))
dendrogram(Z,
           orientation="top",
           distance_sort='descending',
           leaf_rotation=90,
           show_leaf_counts=True,
           labels = locations,
           leaf_font_size=15)
plt.show()

# Given the number of clusters (clusterCount), this program clusters locations
# Shows clustered locations on a scatter diagram
clusterCount = 5
clusterIdToLocations = {}
entireDataSetWithClusterIds =[]
entireDataSetWithClusterIds.append(entireDataSet[0]+["ClusterID"])

clusterIds = fcluster(Z, t=clusterCount, criterion="maxclust")
# print(clusterIds)

for index, clusterId in enumerate(clusterIds):
    if clusterId in clusterIdToLocations:
        clusterIdToLocations[clusterId].append(locations[index])
    else:
        clusterIdToLocations[clusterId] = [locations[index]]
pprint.pprint(clusterIdToLocations)

for i, clusterId in enumerate(clusterIds):
    entireDataSetWithClusterIds.append(entireDataSet[i+1]+[clusterId])

with open(outputFileName2, 'w') as f:
    writer = csv.writer(f)
    writer.writerows(entireDataSetWithClusterIds)

df2 = pd.read_csv(outputFileName2)
scatterFig2= px.scatter_3d(df2,
                           x=entireDataSetWithClusterIds[0][3],
                           y=entireDataSetWithClusterIds[0][4],
                           z=entireDataSetWithClusterIds[0][5],
                           hover_name=entireDataSetWithClusterIds[0][0],
                           color="ClusterID")
scatterFig2.update_layout(height=800)
scatterFig2.show()
